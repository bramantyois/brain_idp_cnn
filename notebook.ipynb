{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sfcn import SFCN\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from volumedatagenerator import VolumeDataGeneratorRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = pd.read_csv('fields/subcortical_volumes.csv')['field id'].to_list()\n",
    "\n",
    "columns=['path']\n",
    "for col in col_list:\n",
    "    columns.append(str(col)+'-2.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = [182, 218, 182]\n",
    "num_output = len(columns)-1\n",
    "batch_size=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('csv/split_train.csv', index_col='eid')[columns]\n",
    "train_gen = VolumeDataGeneratorRegression(\n",
    "    sample_df=train_df, \n",
    "    batch_size=batch_size, \n",
    "    num_reg_classes=num_output, \n",
    "    dim=input_dim,\n",
    "    output_preprocessing='quantile')\n",
    "\n",
    "scaler_instance = train_gen.get_scaler_instance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = pd.read_csv('csv/split_valid.csv', index_col='eid')[columns]\n",
    "valid_gen = VolumeDataGeneratorRegression(\n",
    "    sample_df=valid_df, \n",
    "    batch_size=batch_size, \n",
    "    num_reg_classes=num_output, \n",
    "    dim=input_dim,\n",
    "    output_scaler=scaler_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n"
     ]
    }
   ],
   "source": [
    "model = SFCN(\n",
    "    input_dim=[182, 218, 182, 1], \n",
    "    output_dim=num_output,\n",
    "    conv_num_filters=[32, 64, 128, 256, 256, 64], \n",
    "    conv_kernel_sizes=[3, 3, 3, 3, 3, 1], \n",
    "    conv_strides=[1, 1, 1, 1, 1, 1],\n",
    "    conv_padding=['same', 'same', 'same', 'same', 'same', 'valid'],\n",
    "    pooling_size=[2, 2, 2, 2, 2],\n",
    "    pooling_type=['max_pool', 'max_pool', 'max_pool', 'max_pool', 'max_pool' ]\n",
    "    batch_norm=True,\n",
    "    dropout=False,\n",
    "    softmax=False,\n",
    "    gpu_num=4,\n",
    "    name='sfcn_quantile'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 182, 218, 182, 1) 0         \n",
      "_________________________________________________________________\n",
      "conv_0 (Conv3D)              (None, 91, 109, 91, 32)   896       \n",
      "_________________________________________________________________\n",
      "batchnorm_0 (BatchNormalizat (None, 91, 109, 91, 32)   128       \n",
      "_________________________________________________________________\n",
      "maxpool_0 (MaxPooling3D)     (None, 45, 54, 45, 32)    0         \n",
      "_________________________________________________________________\n",
      "activation_0 (Activation)    (None, 45, 54, 45, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv_1 (Conv3D)              (None, 23, 27, 23, 64)    55360     \n",
      "_________________________________________________________________\n",
      "batchnorm_1 (BatchNormalizat (None, 23, 27, 23, 64)    256       \n",
      "_________________________________________________________________\n",
      "maxpool_1 (MaxPooling3D)     (None, 11, 13, 11, 64)    0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 11, 13, 11, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv_2 (Conv3D)              (None, 11, 13, 11, 128)   221312    \n",
      "_________________________________________________________________\n",
      "batchnorm_2 (BatchNormalizat (None, 11, 13, 11, 128)   512       \n",
      "_________________________________________________________________\n",
      "maxpool_2 (MaxPooling3D)     (None, 5, 6, 5, 128)      0         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 5, 6, 5, 128)      0         \n",
      "_________________________________________________________________\n",
      "conv_3 (Conv3D)              (None, 5, 6, 5, 256)      884992    \n",
      "_________________________________________________________________\n",
      "batchnorm_3 (BatchNormalizat (None, 5, 6, 5, 256)      1024      \n",
      "_________________________________________________________________\n",
      "maxpool_3 (MaxPooling3D)     (None, 2, 3, 2, 256)      0         \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2, 3, 2, 256)      0         \n",
      "_________________________________________________________________\n",
      "conv_4 (Conv3D)              (None, 2, 3, 2, 256)      1769728   \n",
      "_________________________________________________________________\n",
      "batchnorm_4 (BatchNormalizat (None, 2, 3, 2, 256)      1024      \n",
      "_________________________________________________________________\n",
      "maxpool_4 (MaxPooling3D)     (None, 1, 1, 1, 256)      0         \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1, 1, 1, 256)      0         \n",
      "_________________________________________________________________\n",
      "conv_5 (Conv3D)              (None, 1, 1, 1, 64)       16448     \n",
      "_________________________________________________________________\n",
      "batchnorm_5 (BatchNormalizat (None, 1, 1, 1, 64)       256       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 1, 1, 1, 64)       0         \n",
      "_________________________________________________________________\n",
      "avgpool_1 (AveragePooling3D) (None, 1, 1, 1, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv_7 (Conv3D)              (None, 1, 1, 1, 14)       910       \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 14)                0         \n",
      "=================================================================\n",
      "Total params: 2,952,846\n",
      "Trainable params: 2,951,246\n",
      "Non-trainable params: 1,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(learning_rate=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-09 18:26:59.922864: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_1468\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
      "2021-12-09 18:26:59.936853: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-12-09 18:26:59.955140: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2099975000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "INFO:tensorflow:batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 26 all-reduces with algorithm = nccl, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-09 18:27:12.895487: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-12-09 18:27:14.282136: W tensorflow/stream_executor/gpu/asm_compiler.cc:63] Running ptxas --version returned 256\n",
      "2021-12-09 18:27:14.421731: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code 256, output: \n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2021-12-09 18:27:15.684059: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1569/1569 [==============================] - ETA: 0s - loss: 0.1085 - mae: 0.2716"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-09 18:41:44.207392: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_17151\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1569/1569 [==============================] - 1153s 692ms/step - loss: 0.1085 - mae: 0.2716 - val_loss: 0.0964 - val_mae: 0.2621\n",
      "Epoch 2/40\n",
      "1569/1569 [==============================] - 1050s 668ms/step - loss: 0.0808 - mae: 0.2454 - val_loss: 0.1126 - val_mae: 0.2773\n",
      "Epoch 3/40\n",
      "1569/1569 [==============================] - 1076s 685ms/step - loss: 0.0724 - mae: 0.2306 - val_loss: 0.1819 - val_mae: 0.3410\n",
      "Epoch 4/40\n",
      "1569/1569 [==============================] - 1038s 660ms/step - loss: 0.0613 - mae: 0.2085 - val_loss: 0.1657 - val_mae: 0.3242\n"
     ]
    }
   ],
   "source": [
    "model.train_generator(train_gen, valid_gen, batch_size=batch_size, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights('weights/checkpoint_sfcn_batchnorm')\n",
    "# train_df.iloc[9]['path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.model.evaluate(valid_gen)\n",
    "# train_df['path'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#data_size=200\n",
    "#X = np.random.normal(loc=5, scale=5, size=(data_size, *(input_dim), 1))\n",
    "#y = np.random.normal(loc=1, scale=1, size=(data_size, num_output))\n",
    "#model.train(X, y, batch_size, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_test = np.random.normal(loc=8, scale=3, size=(1, 160, 192, 160, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test = np.mean(x_test, axis=(1,2,3,4))\n",
    "#print(y_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "07a6a1ab2a95e05132d9ffaeb8e54a95a2d3cb5cb59827ad97cccdbb2506f6b0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
