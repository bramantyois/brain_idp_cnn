\documentclass{article}

\usepackage{biblatex}
\addbibresource{bib.bib}

\title{Brain Image-Derived Phenotypes Regression Using Convolutional Neural Network}

\author{Bramantyo Ibrahim Supriyatno}

\date{March 2022}

\begin{document}
    \maketitle
    \begin{abstract}
        The use of convolutional neural networks (CNN) for computer vision problems has been significantly successful. 
        However, there is still a debate whether a complex non-linear model can outperform linear models for brain imaging data. 
        Knowing how each component of CNN affects its performance would be beneficial in order to address this debate. 
        In this project, we examine how each component of CNN contributes to the overall performance of the model in predicting brain image-derived phenotypes(IDPs). 
        We found that kernel size, strides number and deeper network have profound effects on the performance especially in gray-white contrast related phenotypes. 
    \end{abstract}
    

    \section*{Introduction}
    The convolutional neural network (CNN) has been proven very successful for many computer vision applications. 
    AlexNet\cite{alexnet} successfully reached state-of-the-art performance for image classification tasks on the ImageNet dataset. 
    Since then, the popularity of CNN has been rapidly growing.
    
    The use of CNN in brain imaging data has been studied likewise. 
    Wen\cite{wen} reported that three-dimensional structural magnetic resonance image (sMRI) CNN performs quite well for Alzheimer's disease. 
    However, he also reported that the CNN model did not perform better than the support vector machine (SVM) model. 
    Peng\cite{peng} also studied the use of CNN on predicting age. He proposed a shallow and straightforward CNN model that achieved a good result. 
    Schulz\cite{schulz} studied the influence of model complexity and sample size on age prediction. 
    He demonstrated that deep networks did not achieve better results compared to standard machine learning algorithms. 
    However, a study by Abrol\cite{abrol} reported the contrary. 
    He argued that complicated features could be extracted by the presence of nonlinearity in the deep learning algorithm. 
    However the CNN model that he used was slightly different to Peng and Schulz. 
        
    The architectural design differences used in the previous works prohibit an impartial comparison. 
    Therefore, there is a need to systematically study how each component in CNN affected its performance. 
    The effect of kernel size, strides number, number of layers, pooling types and normalization methods are among things that should be studied.

    Furthermore, measuring performance only on one-dimensional prediction task would constrain the CNN to only exploit features that are significant for the task. 
    This would force the algorithm to extract complex features and hinder its ability to distill more universal features. 
    Therefore predicting image-derived phenotypes (IDPs) would be more suitable for this study. 
    IDPs are phenotypes related to volumes, areas, thickness, gray-white contrast, and intensities in a certain part or region of the brain. 
    These phenotypes can be potential biomarkers relating to certain brain conditions. 
    
    The effect of design choices of CNN for has been well studied. 
    Smith\cite{smith} presented a guideline for designing CNN. 
    The guidelines recommended using pyramidal structure, downsampling between blocks, and max-pooling. 
    Lathuiliere\cite{lath} specifically studied the effect on regression tasks. 
    He suggested modifying the last fully connected layer before the output when using state-of-the-art (SOTA) architectures for different applications.
    

    \section*{Method}
    CNN was initially proposed by LeCun\cite{lecun} in his groundbreaking paper. 
    The algorithm combines kernel convolution and gradient-based learning which updates the kernel weights. 
    The kernel convolution method reduces the number of parameters. 
    The use of backpropagation allows kernels to extract similar patterns across datasets. 
    These properties make the algorithm suitable for many computer vision problems such as optical character and object recognition.

    In general, the modern CNN block comprises a convolutional layer, activation function, pooling layer and normalization layer. 
    The convolutional layer has several parameters that dictates how it will be applied to the data. 
    Kernel size determines how big the kernel patch will be. 
    Bigger kernel size would allow more complex patterns in trade of increasing number of parameters. 
    Strides number determine how much overlapping region each convolution will share. 
    Bigger stride number will reduce overlap although a larger number will eventually result in loss of information. 
    Strides number also determines the output size of the convolution. 
    Strides number of 2 will halves the output size.

    Convolutional layer is often followed by pooling layer. 
    The function of the pooling layer is to select or combine the convolution output while reducing the size of it. 
    The downsampling properties of the pooling layer can also be done by setting a proper strides number although algorithmically different. 
    The most common type of pooling is maximum pooling which picks the maximum value. 
    Another type of pooling is average pooling which takes the average value of the area. 

    Batch normalization normalizes the output value across the batch. 
    Batch normalization mitigates the effect of distribution shift across different batches during training. 
    Batch normalization also acts as a regularizer. 
    However, batch normalization requires a rather large number of batches which can be a technical difficulty when having structural-MRI images. 
    The alternatives for smaller batch sizes are group standardization and weight normalization.

    The CNN eventually would be fitted to pairs of structural magnetic resonance images (S-MRI) and IDPs. 
    The S-MRIs are three-dimensional image data with a size of 190x210x190. 
    Preprocessing steps such as standardization and cropping were applied to keep the global mean to zero and throw out zero-valued voxels. 
    The output of the network was a vector of the size of the IDPs values. 
    In this experiment, we picked 1421 IDPS values in which distributions were not too skewed. 
    The values also were transformed by quantile transform with the number of quantiles 1000 following a normal distribution. 

    The S-MRI dataset contained 10320 images which were then split into 60\% of the training set, 20\% of the validation set, and the remaining for the test set. 
    Each model in this experiment was trained using the training set. 
    For each training epoch, mean squared error(MSE) of the validation set was computed and if there was no improvement for at least 8 epochs, the training would stop. 
    The model would be kept when its validation setâ€™s MSE was the lowest.

    The experiments were conducted in a GPU cluster with 8 GTX 1080 ti totaling almost 100 gigabytes of GPU memory. 
    In general, training on 3D S-MRI data was exhaustive in terms of memory and there was a possibility of a bottleneck in data loading. 
    Models were implemented using Keras and Tensorflow.
    \section*{Results}
    lorem
    \section*{Conclussions and Summary}

    \printbibliography
\end{document}